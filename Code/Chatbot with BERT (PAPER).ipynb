{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e10c4d",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834b4df5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NeuralNetwork_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14024/1138595862.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mNeuralNetwork_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'NeuralNetwork_base'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import os\n",
    "from transformers import WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "from NeuralNetwork_base import NeuralNetwork\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "}\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Process:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.current_doc = 0  # to avoid random sentence from same doc\n",
    "\n",
    "        # for loading samples directly from file\n",
    "\n",
    "\n",
    "        # for loading samples in memory\n",
    "        self.current_random_doc = 0\n",
    "        self.num_docs = 0\n",
    "\n",
    "        # load samples into memory\n",
    "        data = open('ubuntu_data/train.txt', 'r').readlines()\n",
    "        data = [sent.split('\\n')[0].split('\\t') for sent in data]\n",
    "        y = [int(a[0]) for a in data]\n",
    "        cr = [[sen for sen in a[1:]] for a in data]\n",
    "        crnew = []\n",
    "        for i, crsingle in enumerate(cr):\n",
    "            if y[i] == 1:\n",
    "                crnew.append(crsingle)\n",
    "        crsets = crnew\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "        special_tokens_dict = {'eos_token': '[eos]'}\n",
    "        num_added_toks = self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "        self.sample_to_doc = []\n",
    "        self.all_docs = []\n",
    "        doc = []\n",
    "        # crsets=crsets[:50000]#crsets[:50000]+crsets[500000:]\n",
    "        cnt = 0\n",
    "        print(\"로드끝\")\n",
    "\n",
    "        for crset in tqdm(crsets):\n",
    "            tempcnt = 0\n",
    "            crset=crset[:-1]\n",
    "            for i, line in enumerate(crset):\n",
    "                if len(line) == 0:\n",
    "                    tempcnt += 1\n",
    "                    continue\n",
    "                if len(line) < 10:\n",
    "                    if len(self.tokenizer.tokenize(line)) == 0:\n",
    "                        # print('\\n'+line+'\\n')\n",
    "                        cnt += 1\n",
    "                        tempcnt += 1\n",
    "                        continue\n",
    "\n",
    "                sample = {\"doc_id\": len(self.all_docs),\n",
    "                          \"line\": len(doc)}\n",
    "                self.sample_to_doc.append(sample)\n",
    "                doc.append(line)\n",
    "\n",
    "            if (len(doc) != 0):\n",
    "                self.all_docs.append(doc)\n",
    "            else:\n",
    "                print(\"empty\")\n",
    "            doc = []\n",
    "        print(cnt)\n",
    "        for doc in self.all_docs:\n",
    "            if len(doc) == 0:\n",
    "                print(\"problem\")\n",
    "        self.num_docs = len(self.all_docs)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_to_doc)\n",
    "\n",
    "\n",
    "    def random_sent(self, index):\n",
    "        sample = self.sample_to_doc[index]\n",
    "        self.current_doc = sample[\"doc_id\"]\n",
    "\n",
    "        context_len=sample[\"line\"]\n",
    "        if context_len==0:\n",
    "            return -1,-1,-1\n",
    "        context=[]\n",
    "        for i in range(context_len):\n",
    "            utterance=self.all_docs[sample[\"doc_id\"]][i]\n",
    "            context+=self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(utterance))+[self.tokenizer.eos_token_id]\n",
    "\n",
    "        utterance = self.all_docs[sample[\"doc_id\"]][context_len]\n",
    "        response= self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(utterance))\n",
    "\n",
    "        utterance=self.get_random_line()\n",
    "        negative = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(utterance))\n",
    "\n",
    "        assert len(context) > 0\n",
    "        assert len(response) > 0\n",
    "        assert len(negative) > 0\n",
    "        return context,response,negative\n",
    "\n",
    "\n",
    "    def get_random_line(self):\n",
    "        for _ in range(10):\n",
    "\n",
    "            rand_doc_idx = random.randint(0, len(self.all_docs)-1)\n",
    "            rand_doc = self.all_docs[rand_doc_idx]\n",
    "            line = rand_doc[random.randrange(len(rand_doc))]\n",
    "            if self.current_random_doc != self.current_doc:\n",
    "                break\n",
    "        return line\n",
    "\n",
    "\n",
    "    def makedata(self,item):\n",
    "\n",
    "        context, response, negative= self.random_sent(item)\n",
    "        if context==-1:\n",
    "            return -1,-1\n",
    "\n",
    "        truecrslist=context+[self.tokenizer.sep_token_id]+response\n",
    "        falsecrlist=context+[self.tokenizer.sep_token_id]+negative\n",
    "        return truecrslist,falsecrlist\n",
    "\n",
    "def data_augmentation():\n",
    "    object=Process()\n",
    "    newdata={}\n",
    "    newdata['y']=[]\n",
    "    newdata['cr']=[]\n",
    "    for i in tqdm(range(object.__len__())):\n",
    "        truecrlist,falsecrlist=object.makedata(i)\n",
    "        if truecrlist==-1:\n",
    "            continue\n",
    "        newdata['y'].append(1)\n",
    "        newdata['cr'].append(truecrlist)\n",
    "        newdata['y'].append(0)\n",
    "        newdata['cr'].append(falsecrlist)\n",
    "\n",
    "    pickle.dump(newdata, file=open(\"augmentation_train.pkl\", 'wb'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ec506e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train_file TRAIN_FILE] [--bert_model BERT_MODEL] [--output_dir OUTPUT_DIR]\n",
      "                             [--max_seq_length MAX_SEQ_LENGTH] [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE] [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--warmup_proportion WARMUP_PROPORTION] [--do_lower_case]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\minkh\\AppData\\Roaming\\jupyter\\runtime\\kernel-19d5da7f-3a35-44bf-a133-ebdf1dfd4586.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "from transformers import BertTokenizer,BertConfig\n",
    "from transformers import BertForPreTraining\n",
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from setproctitle import setproctitle\n",
    "setproctitle('(janghoon) e-commerce_final')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len, encoding=\"utf-8-sig\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "\n",
    "        self.sample_to_doc = [] # map sample index to doc and line\n",
    "\n",
    "        # load samples into memory\n",
    "\n",
    "        self.all_docs = []\n",
    "        doc = []\n",
    "\n",
    "        crsets = pickle.load(file=open(corpus_path, 'rb'))\n",
    "        #crsets=crsets[:50000]#crsets[:50000]+crsets[500000:]\n",
    "        cnt=0\n",
    "        lcnt=0\n",
    "        for crset in tqdm(crsets):\n",
    "            for line in crset:\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                if len(line) < 10:\n",
    "                    if len(self.tokenizer.tokenize(line)) == 0:\n",
    "                        # print('\\n'+line+'\\n')\n",
    "                        cnt += 1\n",
    "                        continue\n",
    "                sample = {\"doc_id\": len(self.all_docs),\n",
    "                        \"line\": len(doc),\n",
    "                        \"end\": 0 ,\n",
    "                        \"linenum\":1\n",
    "                        }\n",
    "                self.sample_to_doc.append(sample)\n",
    "                # if len(self.tokenizer.tokenize(line)) == 0:\n",
    "                # print(\"여기\")\n",
    "                doc.append(line)\n",
    "\n",
    "            if (len(doc) != 0):\n",
    "                self.all_docs.append(doc)\n",
    "            else:\n",
    "                print(\"empty\")\n",
    "\n",
    "            if (len(doc) < 4):\n",
    "                for i in range(len(doc) - 1):\n",
    "                    self.sample_to_doc.pop()\n",
    "\n",
    "                self.sample_to_doc[-1]['end'] = len(doc)\n",
    "            \n",
    "                lcnt+=1\n",
    "            else:\n",
    "                \n",
    "                self.sample_to_doc.pop()\n",
    "                self.sample_to_doc.pop()\n",
    "                self.sample_to_doc.pop()\n",
    "\n",
    "            doc = []\n",
    "        \n",
    "        print(cnt,lcnt)\n",
    "\n",
    "        for doc in self.all_docs:\n",
    "            if len(doc) == 0:\n",
    "                print(\"problem\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.sample_to_doc)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        sample = self.sample_to_doc[item]\n",
    "        # 방법에 비해 문장 길이가 짧은경우.\n",
    "        length = sample['end']\n",
    "\n",
    "        if length != 0:\n",
    "            tokens_a = []\n",
    "            for i in range(length - 1):\n",
    "                tokens_a+=self.tokenizer.tokenize(self.all_docs[sample[\"doc_id\"]][i])+[self.tokenizer.eos_token]\n",
    "            tokens_a.pop()\n",
    "        \n",
    "\n",
    "            #response = self.all_docs[sample[\"doc_id\"]][sample[\"line\"] + length - 1]\n",
    "            rand=random.random()\n",
    "            if rand > 0.75:\n",
    "\n",
    "                # 다음문장\n",
    "                response = self.all_docs[sample[\"doc_id\"]][length - 1]\n",
    "                is_next_label = 2\n",
    "\n",
    "\n",
    "            elif rand > 0.5:\n",
    "\n",
    "                # 네거티브의 반은 그 문장 자체들..즉 context의 문장중 하나임. 그리고 이게 전체 dialog session이라 여긴 괜춘\n",
    "\n",
    "                rand_idx = random.randint(0, length - 2)\n",
    "\n",
    "                response = self.all_docs[sample[\"doc_id\"]][rand_idx]\n",
    "\n",
    "                is_next_label = 1\n",
    "\n",
    "            else:\n",
    "                response = self.get_random_line(sample)\n",
    "                is_next_label = 0\n",
    "\n",
    "            tokens_b = self.tokenizer.tokenize(response)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            t1, t2, t3, t4, is_next_label = self.random_sent(item)\n",
    "            # tokenize\n",
    "            tokens_a = self.tokenizer.tokenize(t1)+[self.tokenizer.eos_token]+self.tokenizer.tokenize(t2)+[self.tokenizer.eos_token]+self.tokenizer.tokenize(t3)\n",
    "            tokens_b = self.tokenizer.tokenize(t4)\n",
    "\n",
    "     \n",
    "        cur_example = InputExample( tokens_a=tokens_a, tokens_b=tokens_b, is_next=is_next_label)\n",
    "\n",
    "        \n",
    "        cur_features = convert_example_to_features(cur_example, self.seq_len, self.tokenizer)\n",
    "\n",
    "        cur_tensors = (torch.tensor(cur_features.input_ids),\n",
    "                       torch.tensor(cur_features.input_mask),\n",
    "                       torch.tensor(cur_features.segment_ids),\n",
    "                       torch.tensor(cur_features.lm_label_ids),\n",
    "                       torch.tensor(cur_features.is_next))\n",
    "\n",
    "        return cur_tensors\n",
    "\n",
    "    def random_sent(self, index):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of two sentences. With prob. 50% these are two subsequent sentences\n",
    "        from one doc. With 50% the second sentence will be a random one from another doc.\n",
    "        :param index: int, index of sample.\n",
    "        :return: (str, str, int), sentence 1, sentence 2, isNextSentence Label\n",
    "        \"\"\"\n",
    "        sample = self.sample_to_doc[index]\n",
    "        t1 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"]]\n",
    "        t2 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"] + 1]\n",
    "        t3 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"] + 2]\n",
    "        rand = random.random()\n",
    "        if rand > 0.75 :\n",
    "            label = 2\n",
    "            t4 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"] + 3]\n",
    "            \n",
    "        elif rand > 0.5:\n",
    "            samedoc = self.all_docs[sample[\"doc_id\"]]\n",
    "            linenum = random.randrange(len(samedoc))\n",
    "\n",
    "            while linenum == sample[\"line\"] + 3:\n",
    "                linenum = random.randrange(len(samedoc))\n",
    "\n",
    "            t4 = samedoc[linenum]\n",
    "            label = 1\n",
    "\n",
    "        else:\n",
    "            t4 = self.get_random_line(sample)\n",
    "            label = 0\n",
    "\n",
    "        assert len(t1) > 0\n",
    "        assert len(t2) > 0\n",
    "        assert len(t3) > 0\n",
    "        assert len(t4) > 0\n",
    "        return t1, t2, t3 ,t4, label\n",
    "\n",
    "    def get_random_line(self,sample):\n",
    "\n",
    "        while(True):\n",
    "            rand_doc_idx = random.randint(0, len(self.all_docs)-1)\n",
    "            if sample[\"doc_id\"]!=rand_doc_idx:\n",
    "                break\n",
    "\n",
    "\n",
    "        rand_doc = self.all_docs[rand_doc_idx]\n",
    "        line = rand_doc[random.randrange(len(rand_doc))]\n",
    "            \n",
    "        return line\n",
    "\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \n",
    "\n",
    "    def __init__(self, tokens_a, tokens_b=None, is_next=None, lm_labels=None):\n",
    "    \n",
    "        self.tokens_a = tokens_a\n",
    "        self.tokens_b = tokens_b\n",
    "        self.is_next = is_next  # nextSentence\n",
    "        self.lm_labels = lm_labels  # masked words for language model\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, is_next, lm_label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_next = is_next\n",
    "        self.lm_label_ids = lm_label_ids\n",
    "\n",
    "\n",
    "def random_word(tokens, tokenizer):\n",
    "  \n",
    "    output_label = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token=='[eos]':\n",
    "            output_label.append(-1)\n",
    "            continue\n",
    "        prob = random.random()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob /= 0.15\n",
    "\n",
    "            # 80% randomly change token to mask token\n",
    "            if prob < 0.8:\n",
    "                tokens[i] = \"[MASK]\"\n",
    "\n",
    "            # 10% randomly change token to random token\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\n",
    "\n",
    "            # -> rest 10% randomly keep current token\n",
    "\n",
    "            # append current token to output (we will predict these later)f\n",
    "            try:\n",
    "                output_label.append(tokenizer.vocab[token])\n",
    "            except KeyError:\n",
    "                # For unknown words (should not occur with BPE vocab)\n",
    "                output_label.append(tokenizer.vocab[\"[UNK]\"])\n",
    "                logger.warning(\"Cannot find token '{}' in vocab. Using [UNK] insetad\".format(token))\n",
    "        else:\n",
    "            # no masking token (will be ignored by loss function later)\n",
    "            output_label.append(-1)\n",
    "\n",
    "    return output_label\n",
    "\n",
    "\n",
    "def convert_example_to_features(example, max_seq_length, tokenizer):\n",
    " \n",
    "    tokens_a = example.tokens_a\n",
    "    tokens_b = example.tokens_b\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "    t1_label = random_word(tokens_a, tokenizer)\n",
    "    t2_label = random_word(tokens_b, tokenizer)\n",
    "    # concatenate lm labels and account for CLS, SEP, SEP\n",
    "    lm_label_ids = ([-1] + t1_label + [-1] + t2_label + [-1])\n",
    "\n",
    "    \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "    if len(tokens_b)==0:\n",
    "        print(example.tokens_b)\n",
    "    assert len(tokens_b) > 0\n",
    "    for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        lm_label_ids.append(-1)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(lm_label_ids) == max_seq_length\n",
    "\n",
    "    if False :\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in tokens]))\n",
    "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        #logger.info(\"LM label: %s \" % (lm_label_ids))\n",
    "        logger.info(\"Is next sentence label: %s \" % (example.is_next))\n",
    "\n",
    "    features = InputFeatures(input_ids=input_ids,\n",
    "                             input_mask=input_mask,\n",
    "                             segment_ids=segment_ids,\n",
    "                             lm_label_ids=lm_label_ids,\n",
    "                             is_next=example.is_next)\n",
    "    return features\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ## Required parameters\n",
    "    parser.add_argument(\"--train_file\",\n",
    "                        default=\"./e_commerce_data/e_commerce_post_train.pkl\",\n",
    "                        type=str,\n",
    "                        help=\"The input train corpus.\")\n",
    "    parser.add_argument(\"--bert_model\", default=\"bert-base-chinese\", type=str,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                             \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "    parser.add_argument(\"--output_dir\",\n",
    "                        default=\"./FPT/PT_checkpoint/e_commerce\",\n",
    "                        type=str,\n",
    "                        help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--max_seq_length\",\n",
    "                        default=240,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "    parser.add_argument(\"--train_batch_size\",\n",
    "                        default=50,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for training.\")\n",
    "    \n",
    "    parser.add_argument(\"--learning_rate\",\n",
    "                        default=3e-5,\n",
    "                        type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--num_train_epochs\",\n",
    "                        default=2.0,\n",
    "                        type=float,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.01,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "\n",
    "    parser.add_argument(\"--do_lower_case\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"Number of updates steps to accumualte before performing a backward/update pass.\")\n",
    "   \n",
    "    args = parser.parse_args()\n",
    "\n",
    "  \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n",
    "\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    special_tokens_dict = {'eos_token': '[eos]'}\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    bertconfig = BertConfig.from_pretrained(args.bert_model)\n",
    "    model = BertForPreTraining.from_pretrained(args.bert_model, config=bertconfig)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.cls.seq_relationship = nn.Linear(bertconfig.hidden_size, 3)\n",
    "    #load checkpoint here\n",
    "    #model.bert.load_state_dict(state_dict=torch.load(\"douban_final/checkpoint28-455552/bert.pt\"))\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    num_train_steps = None\n",
    "    print(\"Loading Train Dataset\", args.train_file)\n",
    "    train_dataset = BERTDataset(args.train_file, tokenizer, seq_len=args.max_seq_length)\n",
    "    num_train_steps = int(\n",
    "        len(train_dataset) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,lr=args.learning_rate)\n",
    "\n",
    "    global_step = 0\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,num_workers=2)\n",
    "    learning_rate=args.learning_rate\n",
    "    before = 10\n",
    "    for epoch in trange(1, int(args.num_train_epochs) + 1, desc=\"Epoch\"):\n",
    "        tr_loss=0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\",position=0)):\n",
    "            with torch.no_grad():\n",
    "                batch = (item.cuda(device=device) for item in batch)\n",
    "            input_ids, input_mask, segment_ids,lm_label_ids, is_next = batch\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            prediction_scores, seq_relationship_score = model(input_ids=input_ids,attention_mask= input_mask, token_type_ids=segment_ids)\n",
    "            #logits = torch.sigmoid(output[0].squeeze())\n",
    "            if lm_label_ids is not None and is_next is not None:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "                masked_lm_loss = loss_fct(prediction_scores.view(-1, model.config.vocab_size),\n",
    "                                            lm_label_ids.view(-1))\n",
    "                next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 3), is_next.view(-1))\n",
    "                total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss = total_loss\n",
    "            if step%100==0:\n",
    "                print('Batch[{}] - loss: {:.6f}  batch_size:{}'.format(step, loss.item(),args.train_batch_size) )\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            else:\n",
    "                loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                if global_step / num_train_steps < args.warmup_proportion:\n",
    "                    lr_this_step = learning_rate * warmup_linear(global_step/num_train_steps, args.warmup_proportion)\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        averloss=tr_loss/step\n",
    "        print(\"epoch: %d\\taverageloss: %f\\tstep: %d \"%(epoch,averloss,step))\n",
    "        print(\"current learning_rate: \", learning_rate)\n",
    "        if global_step/num_train_steps > args.warmup_proportion and averloss > before - 0.01:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = param_group['lr'] * 0.9\n",
    "                learning_rate = param_group['lr']\n",
    "            print(\"Decay learning rate to: \", learning_rate)\n",
    "\n",
    "        before=averloss\n",
    "\n",
    "        if True:\n",
    "            # Save a trained model\n",
    "            logger.info(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "            checkpoint_prefix = 'checkpoint' + str(epoch)\n",
    "            output_dir = os.path.join(args.output_dir, '{}-{}'.format(checkpoint_prefix, global_step))\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            output_dir1 = output_dir + '/bert.pt'\n",
    "            torch.save(model.bert.state_dict(), output_dir1)\n",
    "\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > 3*len(tokens_b):\n",
    "            tokens_a.pop(0)\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(state_dict=torch.load(path))\n",
    "    if torch.cuda.is_available(): model.cuda()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
