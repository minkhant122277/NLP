{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd0c95e",
   "metadata": {},
   "source": [
    "### citation dataset: Banking77\n",
    "\n",
    "    @inproceedings{Casanueva2020,\n",
    "        author      = {I{\\~{n}}igo Casanueva and Tadas Temcinas and Daniela Gerz and Matthew Henderson and Ivan Vulic},\n",
    "        title       = {Efficient Intent Detection with Dual Sentence Encoders},\n",
    "        year        = {2020},\n",
    "        month       = {mar},\n",
    "        note        = {Data available at https://github.com/PolyAI-LDN/task-specific-datasets},\n",
    "        url         = {https://arxiv.org/abs/2003.04807},\n",
    "        booktitle   = {Proceedings of the 2nd Workshop on NLP for ConvAI - ACL 2020}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89995367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from utils import create_supervised_pair, supervised_contrasive_loss, Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bc77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f465e395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (C:\\Users\\damia\\.cache\\huggingface\\datasets\\banking77\\default\\1.1.0\\aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da6651c513f4958bdddb6728fc5eac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('banking77')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcd9641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10003\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3080\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a654613f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10003\n",
      "})\n",
      "10003\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "print(train_dataset)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76090ea",
   "metadata": {},
   "source": [
    "#### Split Train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48067dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(train_dataset, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de089116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What happened to my top-up? It was all done and now it's gone! Are you having problems with your system?\", \"Why haven't I seen the cash from the cheque I deposited yet?\", 'I will not be able to verify my identity.']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]\n"
     ]
    }
   ],
   "source": [
    "train_text = train[\"text\"]\n",
    "train_label = train[\"label\"]\n",
    "print(train_text[:3])\n",
    "print(sorted(set(train_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a75f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Could I open an account for children?', \"I'd like to open an account for my children. How can I do that?\", 'I think there is a mistake.  I am being charged twice.']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]\n"
     ]
    }
   ],
   "source": [
    "val_text = val[\"text\"]\n",
    "val_label = val[\"label\"]\n",
    "print(val_text[:3])\n",
    "print(sorted(set(val_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2dc48ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10003\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "test_dataset = dataset[\"test\"]\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84cea1b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do I locate my card?', 'I still have not received my new card, I ordered over a week ago.', 'I ordered a card but it has not arrived. Help please!']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[\"text\"]\n",
    "test_label = test_dataset[\"label\"]\n",
    "print(test_text[:3])\n",
    "print(sorted(set(test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20024df",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf505b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self,labels,text,batch_size,repeated_label:bool=False):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size \n",
    "        self.count = 0 \n",
    "        self.repeated_label = repeated_label        \n",
    "        # to use when training with supervise contrastive loss\n",
    "        if self.repeated_label == True:\n",
    "           # write the code here\n",
    "            self.exist_classes = [] \n",
    "            self.label_maps = None \n",
    "            self.ids_maps = []\n",
    "            self.len_data = len(self.labels)\n",
    "            self.count_batch = 0 \n",
    "            self.is_left_batch = False\n",
    "            #print(\"self.len_data \",self.len_data)\n",
    "            #print(\"self.len data\",self.batch_size)\n",
    "            self.max_count = self.len_data // self.batch_size \n",
    "            if self.len_data % self.batch_size !=0:\n",
    "                self.max_count += 1 \n",
    "            print(\"the number of maximum of batching :\",self.max_count)\n",
    "            pass\n",
    "          \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # write code here for 1)\n",
    "        if self.repeated_label == True:\n",
    "            self.count +=1  \n",
    "            # it would be clear after call til batch_size  \n",
    "            self.exist_classes.append(self.labels[idx])\n",
    "            self.ids_maps.append(idx)\n",
    "            if self.count_batch == self.max_count - 1:\n",
    "                self.count_batch = +1 \n",
    "                #print(\"self.count_batch :\",self.count_batch)\n",
    "                self.count_batch = 0 \n",
    "\n",
    "                if self.len_data % self.batch_size !=0: \n",
    "                    self.batch_size = self.len_data % self.batch_size\n",
    "                    self.is_left_batch = True\n",
    "                #print(\"change batch size !\",self.batch_size)\n",
    "                #print(\"LAST batching !\")\n",
    "            if self.count == self.batch_size:\n",
    "                unique_labels_keys = list(set(self.exist_classes))\n",
    "                table = [0] * len(unique_labels_keys)\n",
    "                unique_labels = dict(zip(unique_labels_keys,table))\n",
    "                if self.is_left_batch == True:\n",
    "                    self.is_left_batch = False\n",
    "                    self.batch_size = 16  \n",
    "                else: \n",
    "                    self.count_batch += 1\n",
    "                    #print(\"count_batch :\",self.count_batch)          \n",
    "                for class_key in self.exist_classes:\n",
    "                    unique_labels[class_key] = +1 \n",
    "                #print(\"tables of each labels :\",unique_labels)\n",
    "                for index, key  in enumerate(unique_labels):\n",
    "                    if unique_labels[key] > 1:\n",
    "                        print(\"v>1 :\",unique_labels[key])\n",
    "                        break\n",
    "                    if index == len(unique_labels.keys()) - 1:\n",
    "                        while True:\n",
    "                            pos_idx = random.randint(0,self.len_data-1) \n",
    "                            if self.labels[pos_idx] in unique_labels.keys():\n",
    "                                if self.labels[pos_idx] == self.labels[idx]:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                   #print(\"old idx :\",idx,self.labels[idx])\n",
    "                                    idx = pos_idx\n",
    "                                   #print(\"new idx :\",idx,self.labels[idx])\n",
    "                                    unique_labels[self.labels[idx]] +=1  \n",
    "                                   #print(\"statistics tables :\",unique_labels)\n",
    "                                   # replace last token\n",
    "                                    self.exist_classes[-1] = self.labels[idx]\n",
    "                                    if len(set(self.exist_classes)) ==  len(self.exist_classes):\n",
    "                                        print(\"unique_labels:\")\n",
    "                                        #print(unique_labels)\n",
    "                                    self.count = 0  \n",
    "                                    self.exist_classes = [] \n",
    "                                    self.ids_maps = []\n",
    "                                    break \n",
    "\n",
    "        label = self.labels[idx]\n",
    "        data = self.text[idx]\n",
    "        sample = {\"Class\": label,\"Text\": data}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2e5b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "data = []\n",
    "labels = []\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "embed_dim = 768\n",
    "batch_size = 4\n",
    "lr= 1e-5  # you can adjust \n",
    "temp = 0.3  # you can adjust \n",
    "lamda = 0.01  # you can adjust  \n",
    "skip_time = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc3a116e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of maximum of batching : 2001\n",
      "the number of maximum of batching : 501\n",
      "the number of maximum of batching : 770\n"
     ]
    }
   ],
   "source": [
    "train_data = CustomTextDataset(train_label, train_text, batch_size = batch_size, repeated_label = True)\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "val_data = CustomTextDataset(val_label,val_text, batch_size = batch_size, repeated_label = True)\n",
    "val_loader = DataLoader(val_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_data = CustomTextDataset(test_label, test_text, batch_size = batch_size, repeated_label = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e73bff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76}\n"
     ]
    }
   ],
   "source": [
    "num_class = len(np.unique(np.array(train_label)))\n",
    "unique_label = np.unique(np.array(train_label))\n",
    "\n",
    "label_maps = {unique_label[i]: i for i in range(len(unique_label))}\n",
    "\n",
    "print(label_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938ef3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36075eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# download config of Roberta config \n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\",output_hidden_states=True)\n",
    "\n",
    "#chnage modifying the number of classes\n",
    "config.num_labels = num_class\n",
    "# Download pretrain models weight \n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "# change from binary classification to muli-classification and loss automatically change to cross entropy loss\n",
    "model.num_labels = config.num_labels\n",
    "# change the output of last layer to num_class that we want to predict\n",
    "model.classifier.out_proj = nn.Linear(in_features=embed_dim,out_features=num_class)\n",
    "# move to model to device that we set\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea167b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damia\\pythonDSAI\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Using adam optimizer \n",
    "optimizer= AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e723ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, 2000] \n",
      "Train loss: 0.42 \n",
      "Train Acc: 18.99 %\n"
     ]
    }
   ],
   "source": [
    "# this code training models on Cross entropy loss\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "best_valid_loss = float('inf')\n",
    "total_acc_train = 0\n",
    "n_correct = 0 \n",
    "n_wrong = 0\n",
    "\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for (idx, batch) in enumerate(train_loader):\n",
    "        sentence = batch[\"Text\"]\n",
    "        inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "        \n",
    "        # move parameter to device\n",
    "        inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "        # map string labels to class index\n",
    "        labels = [label_maps[stringtoId.item()] for stringtoId in (batch['Class'])]\n",
    "                    \n",
    "        #print(\"show out: \",np.unique(labels, return_counts=True))\n",
    "        # convert list to tensor\n",
    "        labels = torch.tensor(labels).unsqueeze(0)\n",
    "        labels = labels.to(device)\n",
    "#         print(labels)\n",
    "\n",
    "        #(batch_size, seq_len)\n",
    "        #print(inputs[\"input_ids\"].shape)\n",
    "\n",
    "         # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**inputs,labels=labels)\n",
    "        # you can check \n",
    "        loss, logits = outputs[:2]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # write code here \n",
    "        pred_class = torch.argmax(logits)\n",
    "#         print(pred_class)\n",
    "        if pred_class in labels:\n",
    "            n_correct += 1\n",
    "        else:\n",
    "            n_wrong += 1\n",
    "        total_acc_train = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        train_accuracy.append(total_acc_train)\n",
    "        \n",
    "        # to save model eg. model.pth look at pytorch document how to save model\n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             torch.save(model.state_dict(), 'A6-model.pt')\n",
    "        \n",
    "        print(f'[Epoch: {epoch + 1}, {idx}] \\nTrain loss: {loss.item():.2f} \\nTrain Acc: {total_acc_train*100:.2f} %')\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d243a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, 499] \n",
      "Valid loss: 1.23 \n",
      "Valid Acc: 24.20 %\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# this code training models on Cross entropy loss\n",
    "\n",
    "valid_loss = []\n",
    "valid_accuracy = []\n",
    "total_acc_valid = 0\n",
    "best_valid_loss = float('inf')\n",
    "n_correct = 0 \n",
    "n_wrong = 0\n",
    "\n",
    "for epoch in range(1):# loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(val_loader):\n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId.item()] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            #print(\"show out: \",np.unique(labels, return_counts=True))\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            #(batch_size, seq_len)\n",
    "            #print(inputs[\"input_ids\"].shape)\n",
    "\n",
    "             # zero the parameter gradients\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs,labels=labels)\n",
    "            # you can check \n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            pred_class = torch.argmax(logits)\n",
    "#         print(pred_class)\n",
    "            if pred_class in labels:\n",
    "                n_correct += 1\n",
    "            else:\n",
    "                n_wrong += 1\n",
    "            total_acc_valid = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_accuracy.append(total_acc_valid)\n",
    "\n",
    "            # write code here \n",
    "            # to save model eg. model.pth look at pytorch document how to save model\n",
    "\n",
    "#             if loss.item() < best_valid_loss:\n",
    "#                 best_valid_loss = loss.item()\n",
    "#                 torch.save(model.state_dict(), 'A6-model.pt')\n",
    "\n",
    "\n",
    "            print(f'[Epoch: {epoch + 1}, {idx}] \\nValid loss: {loss.item():.2f} \\nValid Acc: {total_acc_valid*100:.2f} %')\n",
    "            \n",
    "            print(running_loss)  \n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d251b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
